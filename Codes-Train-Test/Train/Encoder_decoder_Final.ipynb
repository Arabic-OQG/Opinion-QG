{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5466,
     "status": "ok",
     "timestamp": 1661676486876,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "LgSsVZBfRV0x",
    "outputId": "6440b696-f0c3-4706-89d7-a8cbe33d6989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 140496,
     "status": "ok",
     "timestamp": 1661676469103,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "nGP_s1aMR3jp",
    "outputId": "d0781211-556f-4713-b87f-f0e14b905278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
      "Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
      "  Would remove:\n",
      "    /usr/local/bin/estimator_ckpt_converter\n",
      "    /usr/local/bin/import_pb_to_tensorboard\n",
      "    /usr/local/bin/saved_model_cli\n",
      "    /usr/local/bin/tensorboard\n",
      "    /usr/local/bin/tf_upgrade_v2\n",
      "    /usr/local/bin/tflite_convert\n",
      "    /usr/local/bin/toco\n",
      "    /usr/local/bin/toco_from_protos\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.8.2+zzzcolab20220719082949.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow==2.2\n",
      "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 4.5 kB/s \n",
      "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 47.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.17.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.3.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.0)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 1.2 MB/s \n",
      "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 66.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.47.0)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 56.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.37.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.21.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (57.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2022.6.15)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.2.0)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, scipy, h5py, gast, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.8.0\n",
      "    Uninstalling tensorflow-estimator-2.8.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.8.0\n",
      "    Uninstalling tensorboard-2.8.0:\n",
      "      Successfully uninstalled tensorboard-2.8.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.1.0\n",
      "    Uninstalling h5py-3.1.0:\n",
      "      Successfully uninstalled h5py-3.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.3\n",
      "    Uninstalling gast-0.5.3:\n",
      "      Successfully uninstalled gast-0.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.4.1 which is incompatible.\n",
      "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
      "jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
      "jax 0.3.14 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\u001b[0m\n",
      "Successfully installed gast-0.3.3 h5py-2.10.0 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
      "Found existing installation: keras 2.8.0\n",
      "Uninstalling keras-2.8.0:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.7/dist-packages/keras-2.8.0.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/keras/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled keras-2.8.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting keras==2.1.2\n",
      "  Downloading Keras-2.1.2-py2.py3-none-any.whl (304 kB)\n",
      "\u001b[K     |████████████████████████████████| 304 kB 5.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (6.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.4.1)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.1.2\n",
      "Found existing installation: numpy 1.21.6\n",
      "Uninstalling numpy-1.21.6:\n",
      "  Would remove:\n",
      "    /usr/bin/f2py\n",
      "    /usr/local/bin/f2py\n",
      "    /usr/local/bin/f2py3\n",
      "    /usr/local/bin/f2py3.7\n",
      "    /usr/local/lib/python3.7/dist-packages/numpy-1.21.6.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libgfortran-2e0d59d6.so.5.0.0\n",
      "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libopenblasp-r0-2d23e62b.3.17.so\n",
      "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libquadmath-2d0c479f.so.0.0.0\n",
      "    /usr/local/lib/python3.7/dist-packages/numpy/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled numpy-1.21.6\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting numpy==1.19\n",
      "  Downloading numpy-1.19.0-cp37-cp37m-manylinux2010_x86_64.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 4.5 MB/s \n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.0 which is incompatible.\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.4.1 which is incompatible.\n",
      "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
      "jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
      "jax 0.3.14 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
      "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.19.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install tensorflow==2.2\n",
    "!pip uninstall keras\n",
    "!pip install keras==2.1.2\n",
    "!pip uninstall numpy\n",
    "!pip install numpy==1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CxZ5JCfR3mA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3499,
     "status": "ok",
     "timestamp": 1661676490372,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "28gTLtIyR3om",
    "outputId": "c3026fd7-d39e-480e-8ea0-d1eda9cb3488"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import io\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1661676490776,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "bxXyvLlwR3rO",
    "outputId": "b8ceced3-6116-4cab-e2a6-392c797cfe48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['عليك مراجعه جراح عام وعمل صوره رنين مغناطيسي للبطن والتاكد من وضع القنوات الصفراويه احيانا يحصل الالم بعد عمليه المنظار بسبب وجود الهواء في البطن ويزول مع الوقت',\n",
       "  'اللي بيحس بالم بعد عمليه استئصال مراره سببه ايه بيكون الم في منطقه المراره وفي اسفل الجهه اليمنى من البطن'],\n",
       " ['لا علاقه للسكر او الكلسترول بهذه الالام اما باقي فقد تمت الاجابه عنه في نفس المكرر',\n",
       "  'قريبتي تشتكي من الام اسفل الظهر بقرب عظمه الحوض وتشعر ايضا بنغزات في الثدي مااسباب هذا الالم وهل للسكر والكلسترول سبب في هذا'],\n",
       " ['النهايات العصبيه في الاطراف العلويه والسفليه دقيقه وعند البعض تتاثر بالبروده وعند اخرين بالسخونه وخاصه لدى مرضى السكري او من يعانون من اعتلال عصبي محيطي اذا كانت الحاله مزعجه بشكل كبير يمكن عمل تخطيط اعصاب للاطراف العلويه للتاكد من عدم وجود انضغاط عصبي في اليدين',\n",
       "  'اشعر بتنميل في اطراف الاصابع عندما تلامس المياه وخصوصا المياه الساخنه فما السبب'],\n",
       " ['قد يكون ما تعاني منه هو حب الشباب الدهني ولكن لابد من الفحص السرير لتاكيد التشخيص',\n",
       "  'اريد الاستفسار عن الحبوب على الجسم على الظهر و الصدر و اليدين هي حبوب حمراء و عليها نقط سوداء و تحك و لاتزول علاماتها عن الجلد'],\n",
       " ['نرجو منك التوضيح هل تقصد الجدري ام الجدري المائي في انتظار اجاباتك لنتمكن من افادتك بشكل سليم',\n",
       "  'بنتى عندها 7 سنوات وجلها مرض الجدرى ممكن اعرف العلاج والمضاعفات']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = \"/content/drive/MyDrive/Dataset/Train_new.txt\"\n",
    "# Preview the dataset\n",
    "lines = io.open(path_to_file, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "[[w for w in l.split('\\t')[:2]]  for l in lines[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeSciqwOR3tm"
   },
   "outputs": [],
   "source": [
    "word_pairs = [[w for w in l.split('\\t')[:2]]  for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1661676490777,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "pBERsIArdh5n",
    "outputId": "d768eb78-a1b5-413c-ebbd-81ebccc4c149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10233"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxqKM5v4R3wL"
   },
   "outputs": [],
   "source": [
    "texts_1 = [word_pairs[i][0] for i in range(len(word_pairs))]\n",
    "texts_2 = [word_pairs[i][-1] for i in range(len(word_pairs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1661676491050,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "9_V8EGoTeJhk",
    "outputId": "19472a40-7253-4474-f4c3-27cd6761cd6e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'لا علاقه للسكر او الكلسترول بهذه الالام اما باقي فقد تمت الاجابه عنه في نفس المكرر'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1661676491051,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "7SowNFmseJmz",
    "outputId": "1ae47b20-8d28-4ea2-945a-29ba3e1bd081"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'قريبتي تشتكي من الام اسفل الظهر بقرب عظمه الحوض وتشعر ايضا بنغزات في الثدي مااسباب هذا الالم وهل للسكر والكلسترول سبب في هذا'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMnjqop-R3yl"
   },
   "outputs": [],
   "source": [
    "#PreTreatment\n",
    "\n",
    "def preTreatement(w):\n",
    "  \n",
    "  w = w.lower().strip()\n",
    "\n",
    "  # unicode to ascii\n",
    "  w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
    "  #delete extra white space at the end & begining\n",
    "  w = w.strip()\n",
    "\n",
    "  # add the start and end tokens\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "    \n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwfbNKudR303"
   },
   "outputs": [],
   "source": [
    "# making the pre treatement to the sentences, and return the dataset in the format : answers, questions\n",
    "def extractSentences():\n",
    "\n",
    "  answer = [preTreatement(w) for w in texts_1]\n",
    "  question = [preTreatement(w) for w in texts_2]\n",
    "\n",
    "\n",
    "  return answer, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okQKqiCSR33f"
   },
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjiCfvQpR36H"
   },
   "outputs": [],
   "source": [
    "# Load the dataset + Tokenization for the inputs and outputs\n",
    "#input= Answer\n",
    "#target= Question\n",
    "def loadSentences():\n",
    "    \n",
    "  # creating cleaned input, output pairs\n",
    "  inputLang, targetLang = extractSentences()\n",
    "\n",
    "  inputTensor, inputLang_tokenizer = tokenize(inputLang)\n",
    "  targetTensor, targetLang_tokenizer = tokenize(targetLang)\n",
    "\n",
    "  return inputTensor, targetTensor, inputLang_tokenizer, targetLang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNSZxkx2R38r"
   },
   "outputs": [],
   "source": [
    "# Load input and output tensors\n",
    "\n",
    "inputTensor, targetTensor, inputLang, targetLang = loadSentences()\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "maxLengthTarget, maxLengthInput = targetTensor.shape[1], inputTensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1661676504122,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "qmKyG2vHR3_B",
    "outputId": "e7eed2f9-5470-4c1a-b260-cc4c2a96ce3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10233 10233\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split (you must also leave another 20% aside for the test ... we have left this 20% in a separate file)\n",
    "\n",
    "#inputTensor_train, inputTensor_val, targetTensor_train, targetTensor_val = train_test_split(inputTensor, targetTensor, test_size=0.2)\n",
    "inputTensor_train = inputTensor\n",
    "targetTensor_train = targetTensor\n",
    "# Show length\n",
    "print(len(inputTensor_train), len(targetTensor_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKWGs5jpR4Bi"
   },
   "outputs": [],
   "source": [
    "#Convert from index to word\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1661676504943,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "wWGu-qNcR4D_",
    "outputId": "27e0f070-9615-40b8-f4c5-dbb4b6250ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (ANSWER); index to word mapping\n",
      "1 ----> <start>\n",
      "61 ----> عليك\n",
      "23 ----> مراجعه\n",
      "129 ----> جراح\n",
      "214 ----> عام\n",
      "196 ----> وعمل\n",
      "114 ----> صوره\n",
      "440 ----> رنين\n",
      "667 ----> مغناطيسي\n",
      "1137 ----> للبطن\n",
      "880 ----> والتاكد\n",
      "3 ----> من\n",
      "252 ----> وضع\n",
      "2437 ----> القنوات\n",
      "3837 ----> الصفراويه\n",
      "362 ----> احيانا\n",
      "621 ----> يحصل\n",
      "79 ----> الالم\n",
      "21 ----> بعد\n",
      "107 ----> عمليه\n",
      "881 ----> المنظار\n",
      "87 ----> بسبب\n",
      "33 ----> وجود\n",
      "2281 ----> الهواء\n",
      "4 ----> في\n",
      "215 ----> البطن\n",
      "4273 ----> ويزول\n",
      "17 ----> مع\n",
      "283 ----> الوقت\n",
      "2 ----> <end>\n",
      "\n",
      "Target (QUESTION); index to word mapping\n",
      "1 ----> <start>\n",
      "968 ----> اللي\n",
      "12576 ----> بيحس\n",
      "118 ----> بالم\n",
      "13 ----> بعد\n",
      "41 ----> عمليه\n",
      "351 ----> استيصال\n",
      "3807 ----> مراره\n",
      "1091 ----> سببه\n",
      "563 ----> ايه\n",
      "2515 ----> بيكون\n",
      "21 ----> الم\n",
      "4 ----> في\n",
      "175 ----> منطقه\n",
      "508 ----> المراره\n",
      "189 ----> وفي\n",
      "95 ----> اسفل\n",
      "245 ----> الجهه\n",
      "134 ----> اليمنى\n",
      "3 ----> من\n",
      "105 ----> البطن\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "#show an example\n",
    "#input= Answer\n",
    "#target= Question\n",
    "print (\"Input (ANSWER); index to word mapping\")\n",
    "convert(inputLang, inputTensor_train[0])\n",
    "print ()\n",
    "print (\"Target (QUESTION); index to word mapping\")\n",
    "convert(targetLang, targetTensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661676505348,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "XWsYbuffR4Gl",
    "outputId": "fe836100-7c05-4799-b004-53e9b7ad6430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 52]), TensorShape([64, 52]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definition of hyper Parameters\n",
    "\n",
    "bufferSize = len(inputTensor_train)\n",
    "batchSize = 64\n",
    "stepsPerEpoch = len(inputTensor_train)//batchSize\n",
    "embeddingDimension = 256\n",
    "units = 1024\n",
    "vocabInputSize = len(inputLang.word_index)+1\n",
    "vocabTargetSize = len(targetLang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputTensor_train, targetTensor_train)).shuffle(bufferSize)\n",
    "dataset = dataset.batch(batchSize, drop_remainder=True)\n",
    "\n",
    "#example of input and target batches\n",
    "\n",
    "exampleInputBatch, exampleTargetBatch = next(iter(dataset))\n",
    "exampleInputBatch.shape, exampleTargetBatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4BI-PjRR4I_"
   },
   "outputs": [],
   "source": [
    "# Encoder class\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embeddingDimension, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embeddingDimension)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4128,
     "status": "ok",
     "timestamp": 1661676510455,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "LilZXfTQR4Ll",
    "outputId": "9fd12570-ada7-432e-9fbb-91770cffc0cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 52, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocabInputSize, embeddingDimension, units, batchSize)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(exampleInputBatch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BttQMLoxR4OR"
   },
   "outputs": [],
   "source": [
    "#Attention mechanism class\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    \n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661676511754,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "NG-yLb0-R4Qv",
    "outputId": "1ff87c7e-4d45-4607-d4bc-e83b169e3e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batchSize, sequence_length, 1) (64, 52, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batchSize, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUCOQEp5R4Tm"
   },
   "outputs": [],
   "source": [
    "#Decoder class\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embeddingDimension, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embeddingDimension)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    \n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    \n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1661676515148,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "ZpazKhZhR4WO",
    "outputId": "a6b4ff4f-a34f-41e9-fa81-577c21ef19ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batchSize, vocab size) (64, 27057)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocabTargetSize, embeddingDimension, units, batchSize)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((batchSize, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batchSize, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1661676517026,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "tN5eezkjR4bq",
    "outputId": "cb6d77f2-6372-4e81-c081-96465869aed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27057\n"
     ]
    }
   ],
   "source": [
    "print(vocabTargetSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1661676517027,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "KCiTT2ojR4eZ",
    "outputId": "280bb9ac-2b88-4b30-eaad-6a9ffe68aa7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24976\n"
     ]
    }
   ],
   "source": [
    "print(vocabInputSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Cp4gXssR4hJ"
   },
   "outputs": [],
   "source": [
    "#Define the optimizer and the loss function\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ie-u40DR4j5"
   },
   "outputs": [],
   "source": [
    "#Checkpoints \n",
    "\n",
    "checkpoint_dir = '/content/drive/MyDrive/train_file/28'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omKVeqfWR4mz"
   },
   "outputs": [],
   "source": [
    "#Training function\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "        \n",
    "    # return the encoder output and the decoder hidden state\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targetLang.word_index['<start>']] * batchSize, 1)\n",
    "\n",
    "    # giving the target as the next input (teacher forcing)\n",
    "    \n",
    "    for t in range(1, targ.shape[1]):\n",
    "        \n",
    "      # passing encoder output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  #calculate the loss\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  #calculate the gradients (for backpropagation + optimizer)\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15836,
     "status": "ok",
     "timestamp": 1661677997675,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "P_yuIVgzR4pO",
    "outputId": "3b064216-ae5a-41a6-9068-e7d86b06d12c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from /content/drive/MyDrive/train_file/27/ckpt-30\n"
     ]
    }
   ],
   "source": [
    "checkpoint.restore(checkpoint_manager .latest_checkpoint)\n",
    "if checkpoint_manager .latest_checkpoint:\n",
    "  print(\"Restored from {}\".format(checkpoint_manager.latest_checkpoint))\n",
    "\n",
    "else:\n",
    "  print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 543604,
     "status": "ok",
     "timestamp": 1661649187822,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "6Fe0nCYYR4r2",
    "outputId": "32dbf983-f641-41db-8505-a37a6ebf8d99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 0 Loss 0.1641\n",
      "Epoch 27 Batch 100 Loss 0.1901\n",
      "Epoch 27 Loss 0.1867\n",
      "Time taken for 1 epoch 135.40615272521973 sec\n",
      "\n",
      "Model and loss saved\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.1313\n",
      "Epoch 28 Batch 100 Loss 0.1984\n",
      "Epoch 28 Loss 0.1530\n",
      "Time taken for 1 epoch 135.6809196472168 sec\n",
      "\n",
      "Model and loss saved\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.1384\n",
      "Epoch 29 Batch 100 Loss 0.1127\n",
      "Epoch 29 Loss 0.1253\n",
      "Time taken for 1 epoch 135.2557783126831 sec\n",
      "\n",
      "Model and loss saved\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0756\n",
      "Epoch 30 Batch 100 Loss 0.1007\n",
      "Epoch 30 Loss 0.1029\n",
      "Time taken for 1 epoch 135.61100697517395 sec\n",
      "\n",
      "Model and loss saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the training \n",
    "\n",
    "# restore the latest checkpoint \n",
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "EPOCHS = 30 \n",
    "start_epoch = 0\n",
    "tic = time.clock()\n",
    "if checkpoint_manager.latest_checkpoint:\n",
    "  start_epoch = int(checkpoint_manager.latest_checkpoint.split('-')[-1])\n",
    "  checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch,EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(stepsPerEpoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "    \n",
    "  # saving a checkpoint of the model after every epoch\n",
    "  \n",
    "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / stepsPerEpoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "  #Save loss value in a txt file after every epoch (Optional)\n",
    "  \n",
    "  loss_file=open(\"/content/drive/MyDrive/Final_loss_NEW.txt\", \"a+\")\n",
    "  loss_file.write(str(total_loss / stepsPerEpoch)+'\\n')\n",
    "  loss_file.close()\n",
    "  \n",
    "  print(\"Model and loss saved\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1661678001357,
     "user": {
      "displayName": "amira rah",
      "userId": "12950964654467906428"
     },
     "user_tz": -60
    },
    "id": "0F1sn_QLR4uv",
    "outputId": "22e3b056-05d4-4f35-f2ec-eef0846154c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe93db436d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing to the inference\n",
    "\n",
    "# restore the latest checkpoint \n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeFpRkmgR4xh"
   },
   "outputs": [],
   "source": [
    "#Generate Questions\n",
    "\n",
    "def evaluate(sentence):\n",
    "\n",
    "  sentence = preTreatement(sentence)\n",
    "\n",
    "  inputs = [inputLang.word_index[i] for i in sentence.split(' ') if i in inputLang.word_index.keys()]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=maxLengthInput, padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targetLang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(maxLengthTarget):\n",
    "        \n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targetLang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targetLang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcW4uUNgR40S"
   },
   "outputs": [],
   "source": [
    "def generate_question(sentence):\n",
    "    \n",
    "  result, sentence = evaluate(sentence)\n",
    "  f_res = result[:-6].rstrip()\n",
    "  return f_res"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNGliuscUoS2Mgc8VYkVmk9",
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
