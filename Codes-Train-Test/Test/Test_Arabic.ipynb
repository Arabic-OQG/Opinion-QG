{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgSsVZBfRV0x",
        "outputId": "88a92338-71d6-4dff-9470-abfbaa3ce539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGP_s1aMR3jp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7b1af4d-ed49-4254-8a7c-2259faa4f26d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.8.2+zzzcolab20220719082949.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.2\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2 MB 4.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.17.3)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.2)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |████████████████████████████████| 454 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.14.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.47.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (0.37.1)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2) (1.1.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.8.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, scipy, h5py, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.4.1 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.14 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 scipy-1.4.1 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
            "Found existing installation: keras 2.8.0\n",
            "Uninstalling keras-2.8.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/keras-2.8.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled keras-2.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.1.2\n",
            "  Downloading Keras-2.1.2-py2.py3-none-any.whl (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.1.2\n",
            "Found existing installation: numpy 1.21.6\n",
            "Uninstalling numpy-1.21.6:\n",
            "  Would remove:\n",
            "    /usr/bin/f2py\n",
            "    /usr/local/bin/f2py\n",
            "    /usr/local/bin/f2py3\n",
            "    /usr/local/bin/f2py3.7\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy-1.21.6.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libgfortran-2e0d59d6.so.5.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libopenblasp-r0-2d23e62b.3.17.so\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy.libs/libquadmath-2d0c479f.so.0.0.0\n",
            "    /usr/local/lib/python3.7/dist-packages/numpy/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled numpy-1.21.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.19\n",
            "  Downloading numpy-1.19.0-cp37-cp37m-manylinux2010_x86_64.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 14.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.0 which is incompatible.\n",
            "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.4.1 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.14 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.0 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.2\n",
        "!pip uninstall keras\n",
        "!pip install keras==2.1.2\n",
        "!pip uninstall numpy\n",
        "!pip install numpy==1.19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CxZ5JCfR3mA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28gTLtIyR3om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7822ee0-f2a1-490f-da75-c4bd029622c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "#import packages\n",
        "\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import io\n",
        "import time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxXyvLlwR3rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beed73e4-fd9b-4708-8810-2207541e9890"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['عليك مراجعه جراح عام وعمل صوره رنين مغناطيسي للبطن والتاكد من وضع القنوات الصفراويه احيانا يحصل الالم بعد عمليه المنظار بسبب وجود الهواء في البطن ويزول مع الوقت',\n",
              "  'اللي بيحس بالم بعد عمليه استئصال مراره سببه ايه بيكون الم في منطقه المراره وفي اسفل الجهه اليمنى من البطن'],\n",
              " ['لا علاقه للسكر او الكلسترول بهذه الالام اما باقي فقد تمت الاجابه عنه في نفس المكرر',\n",
              "  'قريبتي تشتكي من الام اسفل الظهر بقرب عظمه الحوض وتشعر ايضا بنغزات في الثدي مااسباب هذا الالم وهل للسكر والكلسترول سبب في هذا'],\n",
              " ['النهايات العصبيه في الاطراف العلويه والسفليه دقيقه وعند البعض تتاثر بالبروده وعند اخرين بالسخونه وخاصه لدى مرضى السكري او من يعانون من اعتلال عصبي محيطي اذا كانت الحاله مزعجه بشكل كبير يمكن عمل تخطيط اعصاب للاطراف العلويه للتاكد من عدم وجود انضغاط عصبي في اليدين',\n",
              "  'اشعر بتنميل في اطراف الاصابع عندما تلامس المياه وخصوصا المياه الساخنه فما السبب'],\n",
              " ['قد يكون ما تعاني منه هو حب الشباب الدهني ولكن لابد من الفحص السرير لتاكيد التشخيص',\n",
              "  'اريد الاستفسار عن الحبوب على الجسم على الظهر و الصدر و اليدين هي حبوب حمراء و عليها نقط سوداء و تحك و لاتزول علاماتها عن الجلد'],\n",
              " ['نرجو منك التوضيح هل تقصد الجدري ام الجدري المائي في انتظار اجاباتك لنتمكن من افادتك بشكل سليم',\n",
              "  'بنتى عندها 7 سنوات وجلها مرض الجدرى ممكن اعرف العلاج والمضاعفات']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "path_to_file = \"/content/drive/MyDrive/Dataset/Train_new.txt\"\n",
        "# Preview the dataset\n",
        "lines = io.open(path_to_file, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "[[w for w in l.split('\\t')[:2]]  for l in lines[:5]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeSciqwOR3tm"
      },
      "outputs": [],
      "source": [
        "word_pairs = [[w for w in l.split('\\t')[:2]]  for l in lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxqKM5v4R3wL"
      },
      "outputs": [],
      "source": [
        "texts_1 = [word_pairs[i][0] for i in range(len(word_pairs))] #A\n",
        "texts_2 = [word_pairs[i][-1] for i in range(len(word_pairs))] #Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMnjqop-R3yl"
      },
      "outputs": [],
      "source": [
        "#PreTreatment\n",
        "\n",
        "def preTreatement(w):\n",
        "  \n",
        "  w = w.lower().strip()\n",
        "\n",
        "  # unicode to ascii\n",
        "  w = ''.join(c for c in unicodedata.normalize('NFD', w) if unicodedata.category(c) != 'Mn')\n",
        "  #delete extra white space at the end & begining\n",
        "  w = w.strip()\n",
        "\n",
        "  # add the start and end tokens\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "    \n",
        "  return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwfbNKudR303"
      },
      "outputs": [],
      "source": [
        "# making the pre treatement to the sentences, and return the dataset in the format : answers, questions\n",
        "def extractSentences():\n",
        "\n",
        "  answer = [preTreatement(w) for w in texts_1]\n",
        "  question = [preTreatement(w) for w in texts_2]\n",
        "\n",
        "\n",
        "  return answer, question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okQKqiCSR33f"
      },
      "outputs": [],
      "source": [
        "#Tokenization\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjiCfvQpR36H"
      },
      "outputs": [],
      "source": [
        "# Load the dataset + Tokenization for the inputs and outputs\n",
        "#input= Answer\n",
        "#target= Question\n",
        "def loadSentences():\n",
        "    \n",
        "  # creating cleaned input, output pairs\n",
        "  inputLang, targetLang = extractSentences()\n",
        "\n",
        "  inputTensor, inputLang_tokenizer = tokenize(inputLang)\n",
        "  targetTensor, targetLang_tokenizer = tokenize(targetLang)\n",
        "\n",
        "  return inputTensor, targetTensor, inputLang_tokenizer, targetLang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNSZxkx2R38r"
      },
      "outputs": [],
      "source": [
        "# Load input and output tensors\n",
        "\n",
        "inputTensor, targetTensor, inputLang, targetLang = loadSentences()\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "maxLengthTarget, maxLengthInput = targetTensor.shape[1], inputTensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmKyG2vHR3_B"
      },
      "outputs": [],
      "source": [
        "# Creating training and validation sets using an 80-20 split (you must also leave another 20% aside for the test ... we have left this 20% in a separate file)\n",
        "\n",
        "#inputTensor_train, inputTensor_val, targetTensor_train, targetTensor_val = train_test_split(inputTensor, targetTensor, test_size=0.2)\n",
        "inputTensor_train = inputTensor\n",
        "targetTensor_train = targetTensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKWGs5jpR4Bi"
      },
      "outputs": [],
      "source": [
        "#Convert from index to word\n",
        "\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWGu-qNcR4D_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8af6e0-f044-45f3-cc57-759e13a787f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input (ANSWER); index to word mapping\n",
            "1 ----> <start>\n",
            "8 ----> لا\n",
            "86 ----> علاقه\n",
            "4805 ----> للسكر\n",
            "7 ----> او\n",
            "4806 ----> الكلسترول\n",
            "903 ----> بهذه\n",
            "477 ----> الالام\n",
            "64 ----> اما\n",
            "1976 ----> باقي\n",
            "193 ----> فقد\n",
            "1201 ----> تمت\n",
            "391 ----> الاجابه\n",
            "584 ----> عنه\n",
            "4 ----> في\n",
            "392 ----> نفس\n",
            "8402 ----> المكرر\n",
            "2 ----> <end>\n",
            "\n",
            "Target (QUESTION); index to word mapping\n",
            "1 ----> <start>\n",
            "6662 ----> قريبتي\n",
            "2516 ----> تشتكي\n",
            "3 ----> من\n",
            "114 ----> الام\n",
            "95 ----> اسفل\n",
            "167 ----> الظهر\n",
            "8592 ----> بقرب\n",
            "1161 ----> عظمه\n",
            "781 ----> الحوض\n",
            "3129 ----> وتشعر\n",
            "246 ----> ايضا\n",
            "3433 ----> بنغزات\n",
            "4 ----> في\n",
            "234 ----> الثدي\n",
            "6663 ----> مااسباب\n",
            "18 ----> هذا\n",
            "74 ----> الالم\n",
            "17 ----> وهل\n",
            "3130 ----> للسكر\n",
            "8593 ----> والكلسترول\n",
            "66 ----> سبب\n",
            "4 ----> في\n",
            "18 ----> هذا\n",
            "2 ----> <end>\n"
          ]
        }
      ],
      "source": [
        "#show an example\n",
        "#input= Answer\n",
        "#target= Question\n",
        "print (\"Input (ANSWER); index to word mapping\")\n",
        "convert(inputLang, inputTensor_train[1])\n",
        "print ()\n",
        "print (\"Target (QUESTION); index to word mapping\")\n",
        "convert(targetLang, targetTensor_train[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWsYbuffR4Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab295fdc-f572-4962-bad7-28ee18f62c50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 52]), TensorShape([64, 52]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#Definition of hyper Parameters\n",
        "\n",
        "bufferSize = len(inputTensor_train)\n",
        "batchSize = 64\n",
        "stepsPerEpoch = len(inputTensor_train)//batchSize\n",
        "embeddingDimension = 256\n",
        "units = 1024\n",
        "vocabInputSize = len(inputLang.word_index)+1\n",
        "vocabTargetSize = len(targetLang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputTensor_train, targetTensor_train)).shuffle(bufferSize)\n",
        "dataset = dataset.batch(batchSize, drop_remainder=True)\n",
        "\n",
        "#example of input and target batches\n",
        "\n",
        "exampleInputBatch, exampleTargetBatch = next(iter(dataset))\n",
        "exampleInputBatch.shape, exampleTargetBatch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4BI-PjRR4I_"
      },
      "outputs": [],
      "source": [
        "# Encoder class\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embeddingDimension, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embeddingDimension)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LilZXfTQR4Ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503822e0-2c60-4a09-a8dc-1e3bedf33805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 52, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocabInputSize, embeddingDimension, units, batchSize)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(exampleInputBatch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BttQMLoxR4OR"
      },
      "outputs": [],
      "source": [
        "#Attention mechanism class\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    \n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG-yLb0-R4Qv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30dba4dc-bb2e-41da-d98d-666385068fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batchSize, sequence_length, 1) (64, 52, 1)\n"
          ]
        }
      ],
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batchSize, sequence_length, 1) {}\".format(attention_weights.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUCOQEp5R4Tm"
      },
      "outputs": [],
      "source": [
        "#Decoder class\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embeddingDimension, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embeddingDimension)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    \n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    \n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpazKhZhR4WO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24eee174-ddbf-4ffb-9f40-ed971f6ba575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batchSize, vocab size) (64, 27057)\n"
          ]
        }
      ],
      "source": [
        "decoder = Decoder(vocabTargetSize, embeddingDimension, units, batchSize)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((batchSize, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batchSize, vocab size) {}'.format(sample_decoder_output.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN5eezkjR4bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcd99b8-d67c-4223-f607-5050404eb0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27057\n"
          ]
        }
      ],
      "source": [
        "print(vocabTargetSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCiTT2ojR4eZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3abee9-a550-4e78-e2f9-a53cf6599daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24976\n"
          ]
        }
      ],
      "source": [
        "print(vocabInputSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cp4gXssR4hJ"
      },
      "outputs": [],
      "source": [
        "#Define the optimizer and the loss function\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ie-u40DR4j5"
      },
      "outputs": [],
      "source": [
        "#Checkpoints \n",
        "\n",
        "checkpoint_dir = '/content/drive/MyDrive/train_file/28'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F1sn_QLR4uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11fcd9a2-3fbd-48b1-8eba-7c0910f56b84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4d89279e10>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Passing to the inference\n",
        "\n",
        "# restore the latest checkpoint \n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeFpRkmgR4xh"
      },
      "outputs": [],
      "source": [
        "#Generate Questions\n",
        "\n",
        "def evaluate(sentence):\n",
        "\n",
        "  sentence = preTreatement(sentence)\n",
        "\n",
        "  inputs = [inputLang.word_index[i] for i in sentence.split(' ') if i in inputLang.word_index.keys()]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=maxLengthInput, padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targetLang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(maxLengthTarget):\n",
        "        \n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targetLang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targetLang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "  return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcW4uUNgR40S"
      },
      "outputs": [],
      "source": [
        "def generate_question(sentence):\n",
        "    \n",
        "  result, sentence = evaluate(sentence)\n",
        "  f_res = result[:-6].rstrip()\n",
        "  return f_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvNkf0ajR42j"
      },
      "outputs": [],
      "source": [
        "# path to the dataset\n",
        "df1= pd.read_pickle(\"/content/drive/MyDrive/Dataset/Test_new.pkl\")\n",
        "df1.set_axis([\"Answer\", \"Question\"],axis=1,inplace=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute only once to create separate refrences and candidatess files\n",
        "\n",
        "for i in range(len(df1)):\n",
        "    \n",
        "    reference_file=open(\"/content/drive/MyDrive/Refrence_28.txt\", \"a+\")\n",
        "    refrence=df1['Question'][i]\n",
        "    reference_file.write(str(refrence)+'\\n')\n",
        "    reference_file.close()\n",
        "    \n",
        "    candidate_file=open(\"/content/drive/MyDrive/Candidate_28.txt\", \"a+\")\n",
        "    candidate_file.write(generate_question(df1['Answer'][i])+'\\n')\n",
        "    candidate_file.close()\n",
        "   \n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "Uiz6KypLyKU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidte_file= open(\"/content/drive/MyDrive/Candidate_28.txt\", \"r\")\n",
        "\n",
        "for line in candidte_file:\n",
        "   \n",
        "    all_candidates = [line.rstrip('\\n') for line in candidte_file]\n",
        "   "
      ],
      "metadata": {
        "id": "DU2vqL4fyKa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_file= open(\"/content/drive/MyDrive/Refrence_28.txt\", \"r\")\n",
        "for line in reference_file:\n",
        "    all_refrences = [line.rstrip('\\n') for line in reference_file]\n",
        "    "
      ],
      "metadata": {
        "id": "2y7_HLMXyK3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "liste_candidates=[]\n",
        "for words in all_candidates:\n",
        "    liste_candidates.append(nltk.word_tokenize(words))"
      ],
      "metadata": {
        "id": "NZwEnDzqSUbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f436c80-15dc-4a8d-af1e-8f3af41e6ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "liste_ref=[]\n",
        "for words in all_refrences:\n",
        "    liste_ref.append(nltk.word_tokenize(words))"
      ],
      "metadata": {
        "id": "wGrPRUCfSUdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff9316a-ddaa-4b89-c0a8-2fd5c673d4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "liste_refrences=[]\n",
        "liste_of_liste_refrences=[]\n",
        "for words in all_refrences:\n",
        "    liste_refrences.append(nltk.word_tokenize(words))\n",
        "    liste_of_liste_refrences.append(liste_refrences)\n",
        "    liste_refrences=[]"
      ],
      "metadata": {
        "id": "hl8VBm-iSUgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50633bd0-e448-48c5-ea1e-492ab330da14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sacre bleu**"
      ],
      "metadata": {
        "id": "M_Cp01sMV2tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5hcj-DTJKbC",
        "outputId": "3cbe41e4-9c17-4f46-9abc-5d66f240eead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 33.0 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2022.6.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.5 portalocker-2.5.1 sacrebleu-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "#bleu4\n",
        "sacrebleu.corpus_bleu(all_candidates, [all_refrences])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k86i3G5kJlB1",
        "outputId": "898b3701-d2f5-4d7c-8e8d-02e793c20a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 10.03 16.3/11.2/10.3/9.9 (BP = 0.858 ratio = 0.867 hyp_len = 38826 ref_len = 44758)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Torch Metrics**"
      ],
      "metadata": {
        "id": "UnH2FXr0WTsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h50yFQFMv9Q",
        "outputId": "2502ace2-c5ca-43a9-9e72-b485716d2299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 34.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.0)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "liste_refrences2=[]\n",
        "liste_of_liste_refrences2=[]\n",
        "for words in all_refrences:\n",
        "    liste_refrences2.append(words)\n",
        "    liste_of_liste_refrences2.append(liste_refrences2)\n",
        "    liste_refrences2=[]"
      ],
      "metadata": {
        "id": "OUH7WVptMtim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import BLEUScore\n",
        "#bleu4\n",
        "metric = BLEUScore()\n",
        "metric(all_candidates,liste_of_liste_refrences2)"
      ],
      "metadata": {
        "id": "XwZ4hYM2Mshy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea696b4a-a794-4d62-e533-fb4cab3e480b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1002)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BLEU**"
      ],
      "metadata": {
        "id": "sjutLUB-e1HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bleu-1\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "chencherry = SmoothingFunction()\n",
        "Bleu_score1= corpus_bleu(liste_of_liste_refrences,liste_candidates, weights=(1,0,0,0), smoothing_function = chencherry.method5)\n",
        "print(Bleu_score1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTk4mi1xe6ft",
        "outputId": "f7dbc0a7-0074-4944-f9d7-b54b15a6a533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.41145044297262906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bleu-2\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "chencherry = SmoothingFunction()\n",
        "Bleu_score2= corpus_bleu(liste_of_liste_refrences,liste_candidates, weights=(0.5, 0.5,))\n",
        "print(Bleu_score2)"
      ],
      "metadata": {
        "id": "Bv0NOVGuSciN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff259e99-1094-4a81-8e53-d58e090fd96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11603093294865212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bleu-3\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "chencherry = SmoothingFunction()\n",
        "Bleu_score3= corpus_bleu(liste_of_liste_refrences,liste_candidates, weights=(0.333, 0.333, 0.334,))\n",
        "print(Bleu_score3)"
      ],
      "metadata": {
        "id": "GXtxEyU_Sck6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465f71b1-61d1-4717-9549-290e92d49efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10598502504894575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bleu-4\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "chencherry = SmoothingFunction()\n",
        "Bleu_score = corpus_bleu(liste_of_liste_refrences,liste_candidates)\n",
        "print(Bleu_score)"
      ],
      "metadata": {
        "id": "YAWB0BL5Scn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1053f775-d36a-44c8-a667-ade373a7c31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10022999924535102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Meteor**"
      ],
      "metadata": {
        "id": "FFO6khviejqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Méthode 1**"
      ],
      "metadata": {
        "id": "TXehZhr0emi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/evaluate@a45df1eb9996eec64ec3282ebe554061cb366388\n",
        "\n"
      ],
      "metadata": {
        "id": "uYhMqIQbSctW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e446d271-7eb8-429a-e352-ed0a8d36ae88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/evaluate@a45df1eb9996eec64ec3282ebe554061cb366388\n",
            "  Cloning https://github.com/huggingface/evaluate (to revision a45df1eb9996eec64ec3282ebe554061cb366388) to /tmp/pip-req-build-jruccqx0\n",
            "  Running command git clone -q https://github.com/huggingface/evaluate /tmp/pip-req-build-jruccqx0\n",
            "  Running command git rev-parse -q --verify 'sha^a45df1eb9996eec64ec3282ebe554061cb366388'\n",
            "  Running command git fetch -q https://github.com/huggingface/evaluate a45df1eb9996eec64ec3282ebe554061cb366388\n",
            "  Running command git checkout -q a45df1eb9996eec64ec3282ebe554061cb366388\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 22.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from evaluate==0.1.3.dev0) (1.19.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from evaluate==0.1.3.dev0) (0.3.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from evaluate==0.1.3.dev0) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from evaluate==0.1.3.dev0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from evaluate==0.1.3.dev0) (4.64.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 71.4 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from evaluate==0.1.3.dev0) (2022.7.1)\n",
            "Collecting huggingface-hub>=0.7.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from evaluate==0.1.3.dev0) (21.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.7/dist-packages (from evaluate==0.1.3.dev0) (4.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.1.3.dev0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.1.3.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.1.3.dev0) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->evaluate==0.1.3.dev0) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate==0.1.3.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate==0.1.3.dev0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate==0.1.3.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->evaluate==0.1.3.dev0) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (2.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=2021.05.0->evaluate==0.1.3.dev0) (1.2.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->evaluate==0.1.3.dev0) (6.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata->evaluate==0.1.3.dev0) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate==0.1.3.dev0) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate==0.1.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate==0.1.3.dev0) (1.15.0)\n",
            "Building wheels for collected packages: evaluate\n",
            "  Building wheel for evaluate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for evaluate: filename=evaluate-0.1.3.dev0-py3-none-any.whl size=55236 sha256=0dced19b9af020f63ed96084a63c7437ba9985feb73ae5a1181922bbef5c2523\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/b8/41/fa312822edc312bb577b68d45e8819a8695dcd2b82e7f3685d\n",
            "Successfully built evaluate\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.4.0 evaluate-0.1.3.dev0 huggingface-hub-0.9.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Méthode 1\n",
        "#FROM :https://huggingface.co/spaces/evaluate-metric/meteor\n",
        "import evaluate\n",
        "meteor = evaluate.load('meteor')\n",
        "predictions = all_candidates\n",
        "references = all_refrences\n",
        "results = meteor.compute(predictions=predictions, references=references)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "JmCaFMVZScw6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139,
          "referenced_widgets": [
            "d4b7189b20e84981a5f48112e4fc6720",
            "ea1dbf4195ad4b9e99ffecf95c343e72",
            "5f57546970e64e2cb216b80fdc26b2e9",
            "149e044890bc4e07b0361e5c1dd9e2b9",
            "87edcd5dc1c14d1da17923b46bdc4f02",
            "b473f116a7bf4b16bb3256b473c7460e",
            "d65244d81eb647f197d7d72d5cfa36a0",
            "da07a0fdbd4e45f49603a21daf0c14b4",
            "6e5429f566eb4d24b6561826ba87fbcd",
            "3fe331d6872d4b53be3601f8f96e2218",
            "ad1f448f135c4a5681655d2f8245c639"
          ]
        },
        "outputId": "73a7f849-d614-4eac-ea3a-ef2f32c59322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.38k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4b7189b20e84981a5f48112e4fc6720"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'meteor': 0.11616491513850566}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Méthode 2**"
      ],
      "metadata": {
        "id": "dTRgJYQJep8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Méthode 2 (Ma fonction)\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def corpus_meteor(expected, predicted):\n",
        "    meteor_score_sentences_list = list()\n",
        "    #Pour calculer le score entre chaque paire\n",
        "    [meteor_score_sentences_list.append(meteor_score(expect, predict)) for expect, predict in zip(expected, predicted)]\n",
        "    #la moyenne de la somme des scores de chaque paire\n",
        "    meteor_score_res = np.mean(meteor_score_sentences_list)\n",
        "    return meteor_score_res\n",
        "\n",
        "Score_meteor= corpus_meteor(liste_of_liste_refrences,liste_candidates)\n",
        "print(Score_meteor)"
      ],
      "metadata": {
        "id": "OaEYkE9_SUmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b54f646-7cbc-4864-ab86-0d2598849192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11616491513850566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rouge**"
      ],
      "metadata": {
        "id": "UqdhGGpreZKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Méthode 1**\n"
      ],
      "metadata": {
        "id": "rVyjwS-ceec7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/pltrdy/rouge\n",
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AJu8m2ldXlp",
        "outputId": "8a3add3c-1384-4060-c449-c035bbf034e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import FilesRouge\n",
        "\n",
        "files_rouge = FilesRouge()\n",
        "hyp_path='/content/drive/MyDrive/Candidate_28.txt'\n",
        "ref_path='/content/drive/MyDrive/Refrence_28.txt'\n",
        "#score total: la moyenne des scores entre chaque paire\n",
        "scores = files_rouge.get_scores(hyp_path, ref_path, avg=True)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ete3_fEhd3H7",
        "outputId": "7353df26-694d-429c-b27e-d77fa1f6e39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.1416801773257511, 'p': 0.2026403721322335, 'f': 0.1540969797109862}, 'rouge-2': {'r': 0.09051456062836127, 'p': 0.10899856895742763, 'f': 0.094455699122403}, 'rouge-l': {'r': 0.13780918910754802, 'p': 0.19454135930585909, 'f': 0.14928038701900248}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Méthode 2**"
      ],
      "metadata": {
        "id": "Yk-ZdF4XetSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://gist.github.com/jamescalam/eca142d1dd7eda4d1dea5182541077cc#file-rouge_avg-ipynb\n",
        "from rouge import Rouge\n",
        "model_out = all_candidates\n",
        "reference = all_refrences\n",
        "rouge = Rouge()\n",
        "rouge.get_scores(model_out, reference, avg=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKurIaj5eyeT",
        "outputId": "07e6b95e-391e-426b-ac54-f5b3d0127b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'r': 0.14175867049878477,\n",
              "  'p': 0.20275263826637877,\n",
              "  'f': 0.15418235199891472},\n",
              " 'rouge-2': {'r': 0.09056470719934652,\n",
              "  'p': 0.1090589559762406,\n",
              "  'f': 0.09450802914961763},\n",
              " 'rouge-l': {'r': 0.13788553768877104,\n",
              "  'p': 0.19464913845228893,\n",
              "  'f': 0.14936309083452548}}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Méthode 3**"
      ],
      "metadata": {
        "id": "nZ_UwvBVewrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/li-plus/rouge-metric\n",
        "!pip install rouge-metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vznFNJmVev5n",
        "outputId": "5767ca9e-12b1-44ea-952f-4eca0109c125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge-metric\n",
            "  Downloading rouge_metric-1.0.1-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 30.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: rouge-metric\n",
            "Successfully installed rouge-metric-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creation de liste de lite non tokenisé pour la ref\n",
        "liste_ref=[]\n",
        "liste_of_liste_ref=[]\n",
        "for i in all_refrences:\n",
        "    liste_ref.append(i)\n",
        "    liste_of_liste_ref.append(liste_ref)\n",
        "    liste_ref=[]"
      ],
      "metadata": {
        "id": "q347bWtSnCCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/li-plus/rouge-metric\n",
        "from rouge_metric import PyRouge\n",
        "\n",
        "\n",
        "hypotheses = all_candidates #[1,2]\n",
        "references = liste_of_liste_ref#[[1],[2]]\n",
        "\n",
        "# Evaluate document-wise ROUGE scores\n",
        "'''rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=False,\n",
        "                rouge_w_weight=1.2, rouge_s=False, rouge_su=False, skip_gap=4)'''\n",
        "\n",
        "rouge = PyRouge(rouge_l=True)\n",
        "scores = rouge.evaluate(hypotheses, references)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koQRoQysqS1l",
        "outputId": "ccebc049-d2ac-4e2a-8120-3377055a62a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.14099389208686308, 'p': 0.17469142358997064, 'f': 0.15604415221745008}, 'rouge-2': {'r': 0.09052464179553243, 'p': 0.09704139263780405, 'f': 0.09366980897598504}, 'rouge-l': {'r': 0.13704700237503206, 'p': 0.16767071018545118, 'f': 0.15082003618314638}}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d4b7189b20e84981a5f48112e4fc6720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea1dbf4195ad4b9e99ffecf95c343e72",
              "IPY_MODEL_5f57546970e64e2cb216b80fdc26b2e9",
              "IPY_MODEL_149e044890bc4e07b0361e5c1dd9e2b9"
            ],
            "layout": "IPY_MODEL_87edcd5dc1c14d1da17923b46bdc4f02"
          }
        },
        "ea1dbf4195ad4b9e99ffecf95c343e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b473f116a7bf4b16bb3256b473c7460e",
            "placeholder": "​",
            "style": "IPY_MODEL_d65244d81eb647f197d7d72d5cfa36a0",
            "value": "Downloading builder script: "
          }
        },
        "5f57546970e64e2cb216b80fdc26b2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da07a0fdbd4e45f49603a21daf0c14b4",
            "max": 2381,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e5429f566eb4d24b6561826ba87fbcd",
            "value": 2381
          }
        },
        "149e044890bc4e07b0361e5c1dd9e2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fe331d6872d4b53be3601f8f96e2218",
            "placeholder": "​",
            "style": "IPY_MODEL_ad1f448f135c4a5681655d2f8245c639",
            "value": " 6.81k/? [00:00&lt;00:00, 184kB/s]"
          }
        },
        "87edcd5dc1c14d1da17923b46bdc4f02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b473f116a7bf4b16bb3256b473c7460e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d65244d81eb647f197d7d72d5cfa36a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da07a0fdbd4e45f49603a21daf0c14b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e5429f566eb4d24b6561826ba87fbcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fe331d6872d4b53be3601f8f96e2218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad1f448f135c4a5681655d2f8245c639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}